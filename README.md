# Multi-Agent Reinforcement Learning: PorÃ³wnanie VDN i QMIX w BenchMARL

## ğŸ“Œ Opis projektu

Celem projektu jest dostosowanie hiperparametrÃ³w i porÃ³wnanie dwÃ³ch popularnych algorytmÃ³w wieloagentowego uczenia ze wzmocnieniem â€” **VDN** (Value Decomposition Networks) i **QMIX** â€” z wykorzystaniem frameworka **BenchMARL**. Eksperymenty przeprowadzono w Å›rodowisku obejmujÄ…cym **scenariusze kooperacyjne** oraz **kompetytywne**.

Projekt zawiera:
- trening agentÃ³w w rÃ³Å¼nych scenariuszach,
- tuning hiperparametrÃ³w (learning rate, discount factor),
- analizÄ™ wynikÃ³w.

---

## ğŸ“¦ Wykorzystane biblioteki

- `BenchMARL`
- `NumPy`, `Pandas`
- `Matplotlib`
- `PyTorch`

---

## ğŸ”¬ Åšrodowiska

### ğŸŸ¢ Multi-Agent Particle Environment (MPE)
Lekka platforma 2D stworzona do testÃ³w MARL.

**Scenariusze:**
- `Simple Spread`: agenci rozkÅ‚adajÄ… siÄ™ rÃ³wnomiernie.
- `Simple Adversary`: kooperacja i unikanie przeciwnika.
- `Simple Push`: scenariusz rywalizacyjny, w ktÃ³rym agenci konkurujÄ… o przesuniÄ™cie obiektu na okreÅ›lonÄ… pozycjÄ™.

**Cechy:**
- Å›rodowisko dyskretne,
- kompatybilne z OpenAI Gym,
- moÅ¼liwoÅ›Ä‡ tworzenia wÅ‚asnych scenariuszy.

---

## ğŸ§  Opis algorytmÃ³w

### âœ… Value Decomposition Networks (VDN)

VDN zakÅ‚ada, Å¼e globalna wartoÅ›Ä‡ zespoÅ‚u agentÃ³w \( Q_{\text{tot}} \) moÅ¼e byÄ‡ wyraÅ¼ona jako suma lokalnych wartoÅ›ci \( Q_i \) dla kaÅ¼dego agenta:

\[
Q_{\text{tot}}(\tau, u) = \sum_{i=1}^{n} Q_i(\tau_i, u_i)
\]

**Cechy:**
- prosta struktura,
- szybki trening,
- ograniczenia przy zÅ‚oÅ¼onych zaleÅ¼noÅ›ciach.

---

### âœ… QMIX

QMIX rozszerza VDN przez zastosowanie **nieliniowej, monotonicznej funkcji mieszajÄ…cej**:

\[
\frac{\partial Q_{\text{tot}}}{\partial Q_i} \geq 0
\]

**Cechy:**
- lepsza reprezentacja zÅ‚oÅ¼onych zaleÅ¼noÅ›ci,
- centralizowane trenowanie z zdecentralizowanym wykonaniem,
- wsparcie dla dynamicznych Å›rodowisk.

---

## âš™ï¸ Plan eksperymentÃ³w

- PorÃ³wnanie QMIX i VDN na zadaniach z MPE.
- Testy kooperacyjne: `Simple Spread`.
- Testy kompetytywne: `Simple Push`,  `Simple Adversary`.
- RÃ³Å¼ne konfiguracje hiperparametrÃ³w.

## âš—ï¸ Eksperymenty z hiperparametrami

W celu znalezienia najlepszej konfiguracji dla algorytmÃ³w VDN i QMIX, przeprowadzono seriÄ™ eksperymentÃ³w z rÃ³Å¼nymi wartoÅ›ciami hiperparametrÃ³w. Punktem wyjÅ›cia byÅ‚a domyÅ›lna konfiguracja BenchMARL (przykÅ‚ad poniÅ¼ej):

default_config = {
    'sampling_device': 'cuda',
    'train_device': 'cuda',
    'buffer_device': 'cuda',
    'share_policy_params': True,
    'prefer_continuous_actions': False,
    'collect_with_grad': False,
    'parallel_collection': False,
    'gamma': 0.99,
    'lr': 0.01,
    'adam_eps': 1.0e-8,
    'clip_grad_norm': 0.5,
    'clip_grad_val': None,
    'soft_target_update': True,
    'polyak_tau': 0.005,
    'hard_target_update_frequency': 100,
    'exploration_eps_init': 1.0,
    'exploration_eps_end': 0.05,
    'exploration_anneal_frames': 1000000,
    'max_n_frames': 1000000,
    'on_policy_collected_frames_per_batch': 2048,
    'on_policy_n_envs_per_worker': 1,
    'on_policy_n_minibatch_iters': 4,
    'on_policy_minibatch_size': 64,
    'off_policy_collected_frames_per_batch': 100,
    'off_policy_n_envs_per_worker': 1,
    'off_policy_n_optimizer_steps': 100,
    'off_policy_train_batch_size': 512,
    'off_policy_memory_size': 1000000,
    'off_policy_init_random_frames': 50000,
    'off_policy_use_prioritized_replay_buffer': False,
    'off_policy_prb_alpha': 0.6,
    'off_policy_prb_beta': 0.4,
    'evaluation': True,
    'render': False,
    'evaluation_interval': 10000,
    'evaluation_episodes': 10,
    'evaluation_deterministic_actions': True,
    'project_name': 'benchmarl',
    'create_json': True,
    'save_folder': 'results',
    'restore_file': None,
    'restore_map_location': None,
    'checkpoint_interval': 100,
    'checkpoint_at_end': True,
    'keep_checkpoints_num': 1,
    'max_n_iters': 5000,
    'loggers': [],
}


---

## ğŸ§© WpÅ‚yw hiperparametrÃ³w na algorytmy MARL

### 1. `lr` (Learning Rate)
- **Opis:** OkreÅ›la szybkoÅ›Ä‡ aktualizacji wag modelu podczas trenowania.
- **WpÅ‚yw:** Zbyt wysoka wartoÅ›Ä‡ moÅ¼e prowadziÄ‡ do niestabilnoÅ›ci modelu, co objawia siÄ™ duÅ¼ymi wahania w nagrodach. Zbyt niska wartoÅ›Ä‡ moÅ¼e spowodowaÄ‡ bardzo wolne uczenie siÄ™ i zbyt maÅ‚e zmiany w polityce agenta.

### 2. `clip_grad_norm`
- **Opis:** Ogranicza dÅ‚ugoÅ›Ä‡ gradientu.
- **WpÅ‚yw:** Zapobiega eksplozji gradientÃ³w, co moÅ¼e byÄ‡ szczegÃ³lnie problematyczne w zÅ‚oÅ¼onych Å›rodowiskach. WyÅ‚Ä…czenie tego parametru (`None`) moÅ¼e prowadziÄ‡ do niestabilnych wynikÃ³w.

### 3. `polyak_tau`
- **Opis:** WspÃ³Å‚czynnik miÄ™kkiego aktualizowania wag targetowych w sieci neuronowej.
- **WpÅ‚yw:** WyÅ¼sze wartoÅ›ci przyspieszajÄ… aktualizacjÄ™ wag, co moÅ¼e skutkowaÄ‡ szybszÄ… konwergencjÄ…, ale rÃ³wnieÅ¼ wiÄ™kszymi fluktuacjami w wynikach. Zbyt niskie wartoÅ›ci mogÄ… spowolniÄ‡ adaptacjÄ™ do nowych danych.

### 4. `off_policy_memory_size`
- **Opis:** Rozmiar bufora pamiÄ™ci do przechowywania doÅ›wiadczeÅ„ agenta.
- **WpÅ‚yw:** WiÄ™kszy bufor umoÅ¼liwia lepszÄ… dywersyfikacjÄ™ danych treningowych, co moÅ¼e poprawiÄ‡ jakoÅ›Ä‡ polityki. Zbyt duÅ¼y bufor moÅ¼e jednak spowodowaÄ‡ problemy z pamiÄ™ciÄ… i czasem przetwarzania.

### 5. `off_policy_train_batch_size`
- **Opis:** Liczba prÃ³bek pobieranych z bufora do jednego kroku optymalizacji.
- **WpÅ‚yw:** Mniejsze wartoÅ›ci mogÄ… powodowaÄ‡ wiÄ™kszy szum w gradientach, co utrudnia konwergencjÄ™. Z drugiej strony, zbyt duÅ¼e wartoÅ›ci mogÄ… spowodowaÄ‡ dÅ‚uÅ¼szy czas treningu i zwiÄ™kszone obciÄ…Å¼enie pamiÄ™ci.

### 6. `prefer_continuous_actions`
- **Opis:** Wskazuje, czy model powinien obsÅ‚ugiwaÄ‡ akcje ciÄ…gÅ‚e.
- **WpÅ‚yw:** WÅ‚Ä…czenie tej opcji wpÅ‚ywa na kompatybilnoÅ›Ä‡ z rÃ³Å¼nymi Å›rodowiskami. W przypadku Å›rodowisk z dyskretnymi akcjami moÅ¼e to prowadziÄ‡ do nieoptymalnych strategii.

### 7. `exploration_eps_init`
- **Opis:** PoczÄ…tkowa wartoÅ›Ä‡ parametru eksploracji w strategii Îµ-greedy.
- **WpÅ‚yw:** Im wyÅ¼sza wartoÅ›Ä‡, tym bardziej eksploracyjne dziaÅ‚ania na poczÄ…tku treningu, co moÅ¼e pomÃ³c w odkrywaniu lepszych strategii. Zbyt niski parametr moÅ¼e prowadziÄ‡ do lokalnych minimÃ³w.

### 8. `exploration_anneal_frames`
- **Opis:** Liczba krokÃ³w, po ktÃ³rych wartoÅ›Ä‡ Îµ jest zmniejszana do wartoÅ›ci koÅ„cowej (`exploration_eps_end`).
- **WpÅ‚yw:** KrÃ³tszy okres prowadzi do szybszego przejÅ›cia z eksploracji do eksploatacji, co moÅ¼e byÄ‡ korzystne w prostych Å›rodowiskach, ale w bardziej zÅ‚oÅ¼onych moÅ¼e prowadziÄ‡ do utraty szans na odkrycie lepszych strategii.

---

## ğŸ“ Wnioski

- **StabilnoÅ›Ä‡ algorytmÃ³w:** VDN w wiÄ™kszoÅ›ci przypadkÃ³w wykazuje wiÄ™kszy rozrzut wynikÃ³w (szersze przedziaÅ‚y wartoÅ›ci), podczas gdy QMIX jest bardziej stabilny, co przejawia siÄ™ w wÄ™Å¼szych przedziaÅ‚ach wartoÅ›ci. Oznacza to, Å¼e QMIX moÅ¼e lepiej radziÄ‡ sobie w bardziej zÅ‚oÅ¼onych scenariuszach.

- **Czas treningu:** Åšredni czas potrzebny na uczenie algorytmu QMIX byÅ‚ o 30% wiÄ™kszy niÅ¼ w przypadku VDN. Oznacza to, Å¼e QMIX moÅ¼e wymagaÄ‡ wiÄ™kszych zasobÃ³w obliczeniowych, co warto uwzglÄ™dniÄ‡ przy wyborze algorytmu.

- **WraÅ¼liwoÅ›Ä‡ na hiperparametry:** Oba algorytmy wykazaÅ‚y znacznÄ… wraÅ¼liwoÅ›Ä‡ na wartoÅ›Ä‡ wspÃ³Å‚czynnika uczenia oraz rozmiar partii treningowej. Odpowiednie dostrojenie tych parametrÃ³w jest kluczowe dla osiÄ…gniÄ™cia wysokiej jakoÅ›ci wynikÃ³w.

- **Eksploracja:** ZwiÄ™kszenie zakresu eksploracji przyczyniÅ‚o siÄ™ do lepszego odkrywania strategii, ale jednoczeÅ›nie wprowadzaÅ‚o wiÄ™kszÄ… zmiennoÅ›Ä‡ wynikÃ³w. Sugeruje to, Å¼e istnieje optymalny poziom eksploracji, ktÃ³ry powinien byÄ‡ ustalany w zaleÅ¼noÅ›ci od specyfiki zadania.

- **Optymalizacja architektury:** W przeprowadzonych eksperymentach skupiono siÄ™ na optymalizacji hiperparametrÃ³w, nie ingerujÄ…c w rozmiary architektury sieci neuronowych. To podejÅ›cie pozwala na maksymalne wykorzystanie potencjaÅ‚u modelu, co jest kluczowe dla efektywnoÅ›ci przy niÅ¼szych kosztach obliczeniowych. Mniejsze architektury zapewniajÄ… szybsze trenowanie oraz mniejsze zuÅ¼ycie zasobÃ³w.


